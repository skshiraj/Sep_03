{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.8"},"colab":{"name":"Backpropagation.ipynb","version":"0.3.2","provenance":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"N5QCLaUnrnBV","colab_type":"text"},"source":["# Back Propagation"]},{"cell_type":"markdown","metadata":{"id":"sH5jPVX-rnBd","colab_type":"text"},"source":["## 1. loading of data"]},{"cell_type":"code","metadata":{"id":"y4WT0EJRrnBg","colab_type":"code","colab":{},"outputId":"90804161-647c-4283-a8c2-27188dd451ce"},"source":["import pickle\n","with open('data.pkl', 'rb') as f:\n","    data = pickle.load(f)\n","print(data.shape)\n","X = data[:, :5]\n","y = data[:, -1]\n","print(X.shape, y.shape)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["(506, 6)\n","(506, 5) (506,)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"GIkq6U0SrnBu","colab_type":"text"},"source":["# 2. Computational graph"]},{"cell_type":"markdown","metadata":{"id":"3XnJH-McrnBx","colab_type":"text"},"source":["<img src='https://i.imgur.com/seSGbNS.png'>"]},{"cell_type":"markdown","metadata":{"id":"YRQKYgJrrnB0","colab_type":"text"},"source":["<pre>\n","1. if you observe the graph, we are having input features [f1, f2, f3, f4, f5] and 9 weights [w1, w2, w3, w4, w5, w6,    w7, w8, w9]\n","2. the final output of this graph is a value L which is computed as (Y-Y')^2\n","</pre>"]},{"cell_type":"markdown","metadata":{"id":"inW-os8IrnB3","colab_type":"text"},"source":["### Task 1: Implementing backpropagation and Gradient checking\n","\n","\n","<pre>1. <b>Check this video for better understanding of the computational graphs and back propagation:</b> <a href='https://www.youtube.com/watch?v=i94OvYb6noo#t=1m33s'>https://www.youtube.com/watch?v=i94OvYb6noo</a>\n","</pre>\n","\n","<pre>\n","2. <b>write two functions</b>\n","\n","#you can modify the definition of this function according to your needs\n","<font color='green'>\n","def forward_propagation(X, y, W):\n","        <font color='grey'>\n","        # X: input data point, note that in this assignment you are having 5-d data points\n","        # y: output varible\n","        # W: weight array, its of length 9, W[0] corresponds to w1 in graph, W[1] corresponds to w2 in graph, ..., W[8] corresponds to w9 in graph.\n","        # write code to compute the value of L=(y-y')^2\n","        </font>\n","        return (L, any other variables which you might need to use for back propagation)\n","        <font color='grey'>\n","        # Hint: you can use dict type to store the required intermediate variables \n","        </font>\n","</font>\n","</pre>\n","\n","<pre>\n","# you can modify the definition of this function according to your needs\n","<font color='blue'>\n","def backward_propagation(L, Variables):\n","        <font color='grey'>\n","        # L: the loss we calculated for the current point\n","        # Variables: the outputs of the forward_propagation() function\n","        # write code to compute the gradients of each weight [w1,w2,w3,...,w9]\n","        </font>\n","        return dW\n","        <font color='grey'>\n","        # here dW can be a list, or dict or any other data type wich will have gradients of all the weights\n","        # Hint: you can use dict type to store the required variables \n","        </font>\n","</font>\n","</pre>\n","3. <b> <a href='https://towardsdatascience.com/how-to-debug-a-neural-network-with-gradient-checking-41deec0357a9'>Gradient checking</a></b>:<a href='https://towardsdatascience.com/how-to-debug-a-neural-network-with-gradient-checking-41deec0357a9'>blog link</a> \n","\n","<pre>we know that the derivative of any function is </pre>$$\\lim_{\\epsilon\\to0}\\frac{f(x+\\epsilon)-f(x-\\epsilon)}{2\\epsilon}$$\n","<pre>\n","The definition above can be used as a numerical approximation of the derivative. Taking an epsilon small enough, the calculated approximation will have an error in the range of epsilon squared. \n","\n","In other words, if epsilon is 0.001, the approximation will be off by 0.00001.\n","\n","Therefore, we can use this to approximate the gradient, and in turn make sure that backpropagation is implemented properly. This forms the basis of gradient checking!\n","\n","</pre>\n","\n","<font >\n","lets understand the concept with a simple example:\n","$f(w1,w2,x1,x2)=w_{1}^{2} . x_{1} + w_{2} . x_{2}$ \n","\n","from the above function lets assume $w_{1}=1$, $w_{2}=2$, $x_{1}=3$, $x_{2}=4$ the gradient of $f$ w.r.t $w_{1}$ is\n","\n","\\begin{array} {lcl}\n","\\frac{df}{dw_{1}} = dw_{1} &=&2.w_{1}.x_{1} \\\\& = &2.1.3\\\\& = &6\n","\\end{array}\n","\n","\n","let calculate the aproximate gradient of $w_{1}$ as mentinoned in the above formula and considering $\\epsilon=0.0001$\n","\n","\\begin{array} {lcl}\n","dw_1^{approx} & = & \\frac{f(w1+\\epsilon,w2,x1,x2)-f(w1-\\epsilon,w2,x1,x2)}{2\\epsilon} \\\\ & = & \\frac{((1+0.0001)^{2} . 3 + 2 . 4) - ((1-0.0001)^{2} . 3 + 2 . 4)}{2\\epsilon} \\\\ & = & \\frac{(1.00020001 . 3 + 2 . 4) - (0.99980001. 3 + 2 . 4)}{2*0.0001} \\\\ & = & \\frac{(11.00060003) - (10.99940003)}{0.0002}\\\\ & = & 5.99999999999\n","\\end{array}\n","\n","Then, we apply the following formula for gradient check: <i>gradient_check</i> = \n","$\\frac{\\left\\Vert\\left (dW-dW^{approx}\\rm\\right) \\right\\Vert_2}{\\left\\Vert\\left (dW\\rm\\right) \\right\\Vert_2+\\left\\Vert\\left (dW^{approx}\\rm\\right) \\right\\Vert_2}$\n","\n","The equation above is basically the Euclidean distance normalized by the sum of the norm of the vectors. We use normalization in case that one of the vectors is very small.\n","As a value for epsilon, we usually opt for 1e-7. Therefore, if gradient check return a value less than 1e-7, then it means that backpropagation was implemented correctly. Otherwise, there is potentially a mistake in your implementation. If the value exceeds 1e-3, then you are sure that the code is not correct.\n","\n","in our example: <i>gradient_check</i> $ = \\frac{(6 - 5.999999999994898)}{(6 + 5.999999999994898)} = 4.2514140356330737e^{-13}$\n","\n","you can mathamatically derive the same thing like this\n","\n","\\begin{array} {lcl}\n","dw_1^{approx} & = & \\frac{f(w1+\\epsilon,w2,x1,x2)-f(w1-\\epsilon,w2,x1,x2)}{2\\epsilon} \\\\ & = & \\frac{((w_{1}+\\epsilon)^{2} . x_{1} + w_{2} . x_{2}) - ((w_{1}-\\epsilon)^{2} . x_{1} + w_{2} . x_{2})}{2\\epsilon} \\\\ & = & \\frac{4. \\epsilon.w_{1}. x_{1}}{2\\epsilon} \\\\ & = &  2.w_{1}.x_{1}\n","\\end{array}\n","\n","to do this task you need to write a function \n","<pre>\n","<font color='darkblue'>\n","W = initilize_randomly\n","def gradient_checking(data_point, W):\n","    <font color='grey'>\n","    # compute the L value using forward_propagation()\n","    # compute the gradients of W using backword_propagation()\n","    </font>\n","    approx_gradients = []\n","    for each wi weight value in W:\n","        <font color='grey'>\n","        # add a small value to weight wi, and then find the values of L with the updated weights\n","        # subtract a small value to weight wi, and then find the values of L with the updated weights\n","        # compute the approximation gradients of weight wi\n","        </font>\n","        approx_gradients.append(approximation gradients of weight wi)\n","    <font color='grey'>\n","    # compare the gradient of weights W from backword_propagation() with the aproximation gradients of weights with      gradient_check formula\n","    </font>\n","    return gradient_check\n","</font>\n","NOTE: you can do sanity check by checking all the return values of gradient_checking(), they have to be zero. if not you have bug in your code\n","</pre>"]},{"cell_type":"markdown","metadata":{"id":"mf1fj5ZernB5","colab_type":"text"},"source":["### Task 2: Optimizers\n","\n","1. As a part of this task, you will be implementing 3 type of optimizers(methods to update weight)\n","2. check this video and blog: https://www.youtube.com/watch?v=gYpoJMlgyXA,  http://cs231n.github.io/neural-networks-3/\n","3. use the same computational graph that was mentioned above to do this task\n","4. initilze the 9 weights from normal distribution with mean=0 and std=0.01\n","\n","5. \n","\n","<pre>\n","    for each epoch(1-100):\n","        for each data point in your data:\n","            using the functions forward_propagation() and backword_propagation() compute the gradients of weights\n","            update the weigts with help of gradients  ex: w1 = w1-learning_rate*dw1\n","</pre>\n","\n","6.\n","\n","<pre>\n","<b>task 2.1</b>: you will be implementing the above algorithm with <b>Vanilla update</b> of weights\n","<b>task 2.2</b>: you will be implementing the above algorithm with <b>Momentum update</b> of weights\n","<b>task 2.3</b>: you will be implementing the above algorithm with <b>Adam update</b> of weights\n","</pre>\n","\n"]}]}